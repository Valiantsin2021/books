# Notes

## Sources

[Introduction to Data](https://app.datacamp.com/learn/courses/introduction-to-data)

[Introduction to Data Culture](https://app.datacamp.com/learn/courses/introduction-to-data-culture)

[Introduction to Data Quality](https://app.datacamp.com/learn/courses/introduction-to-data-quality)

[Introduction to Data Security](https://app.datacamp.com/learn/courses/introduction-to-data-security)

[Understanding Data Engineering](https://app.datacamp.com/learn/courses/understanding-data-engineering)

[Understanding Data Science](https://app.datacamp.com/learn/courses/understanding-data-science)

[Understanding Data Visualization](https://app.datacamp.com/learn/courses/understanding-data-visualization)

## [Introduction to Data](https://app.datacamp.com/learn/courses/introduction-to-data)

### Structured vs Unstructured data

Customer info, Financial transactions vs Email, Video

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2t881wvg8r6ouzmhuccf.png)

### Quantitative vs Qualitative data

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9e4zlksqc0eo77v2ohhx.png)

### Data Context

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v1qwhj8tyujpp72on2dn.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rppvutaumamzfdfvqod0.png)

### DIKW pyramid (4 layer model):

1. Data
2. Information
3. Knowledge
4. Wisdom

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/m38nhp3xkoamfk4ly4p2.png)

Start with raw data, organize it into information interpret it into knowledge & wisdom.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/307uvnuxqk92rl3yoc9k.png)

Wisdom: Wisdom means applied knowledge. Because it will probably rain tonight and make the park muddy, James' parents should plan some alternative games for their fifteen quests.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bo8aw9duugmvvxo068ck.png)

### Data decision making

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n1n1fti5knlcsi9d0dxz.png)

Asking the right question will:

- ﻿﻿Outline exact what you are looking to answer
- ﻿﻿Prevent scope creep
- ﻿﻿Ensure success throughout the rest of the process

Gather the right data for your question:

- ﻿﻿Data can live in multiple locations
- ﻿﻿Think ahead to your analysis, you can save time and effort now

Prepare data for analysis:

- ﻿﻿Clean bad to good data
- ﻿﻿Arrange data into expected structure for analysis
- ﻿﻿Sometimes most time consuming task

Analyzing data; decisions are based on data analysis. Here are some tools:

- ﻿﻿Python, R
- ﻿﻿Tableau, Power BI
- ﻿﻿Excel, Google Sheets

Making decisions:

- ﻿﻿The last step of the data-driven process
- ﻿﻿Better decisions based on data, especially when combined with personal experiences
- ﻿﻿Iterative process

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ycnzb27oe9cqlbmt5ca3.png)

- 

### Principles of data ethics

1. ﻿﻿﻿Permission for data collection
2. ﻿﻿﻿Transparency about the plan
3. ﻿﻿﻿Privacy of data
4. ﻿﻿﻿Good intentions
5. ﻿﻿﻿Consider the outcome

### What is the data life cycle?

- ﻿﻿Planning and collecting
- ﻿﻿Storing and managing
- ﻿﻿Cleaning and processing
- ﻿﻿Analyzing and visualizing
- ﻿﻿Sharing
- ﻿﻿Archiving/destroying

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/off9fgvju6c4zty14m30.png)

## [Introduction to Data Culture](https://app.datacamp.com/learn/courses/introduction-to-data-culture)

### Key components of data culture

- Making data-driven decisions
  - ﻿﻿Evaluate strategy
  - ﻿﻿Identify opportunities
  - ﻿﻿Measure success
  - ﻿﻿Evidence over intuition or opinion
- Prioritize data literacy: being able to read, interpret and analyze data
- Continuous experimentation and development

Benefits:

- ﻿﻿Enhanced decision-making

- ﻿﻿Improved operations

- Efficiency boost

  

### Data challenges

#### People related

Data silos:

- Occur when information is isolated within teams or departments
- ﻿﻿Hamper collaboration, limit organizational understanding
- ﻿﻿Example:
  * Ford's siloed data storage
- ﻿﻿Solutions:
  * Centralized data team/data management system

Data literacy:

- ﻿﻿Reluctance to gain new skills and adapt to changing data trend
- ﻿﻿Failure to invest in long-term commitment
- ﻿﻿Example:
  - ﻿﻿Assume all employees have the same level of data literacy
- ﻿﻿Solutions:
  - ﻿﻿Customized training program
  - ﻿﻿Continuously monitoring and refining

﻿﻿Data ethics:

- ﻿﻿Responsible, fair, and transparent data usage
- ﻿﻿Example:
   o Amazon's biased hiring algorithm
- ﻿﻿Solutions:
  - ﻿﻿Establish clear ethical guidelines
  - ﻿﻿Foster open discussion

#### Process related

Data quality:

- ﻿﻿Data needs to be ﻿accurate, reliable, up-to-date

* Example:
  - ﻿﻿Uber's "Greyball" resulted in severe consequences

* ﻿﻿Solutions:

  * Implement data validation process

  - ﻿﻿Leverage automated data cleansing tools

Data privacy & security:

- Extended data usage may lead to overlook of data privacy and data security
- ﻿﻿Protecting sensitive information maintains customer trust and avoid potential breaches
- ﻿﻿Example: 2017 Equifax data breach
- ﻿﻿Solutions:
  - ﻿﻿Implement access controls
  - ﻿﻿Apply encryption techniques
  - ﻿﻿Conduct regular security checks

### 5 level data maturity model

1. Awareness: the organization knows that data is important but lacks a plan for how to understand and analyze its data.
2. Adoption: some stakeholders within the organization have attempted to make data-informed decisions but the approach is often limited or siloed.
3. Standardization: achieved by breaking through the siloes between different pockets of the organization and acting as one unified vision.
4. Optimization: this is when the organizations try to improve efficiency by coordinating between different groups and data stream.
5. Innovation: a data culture is born and data's value, importance, and utility is realized at every level.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6usj9omw2w8svqusypt6.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/b8xrn213txy7hr5nsdme.png)

## [Introduction to Data Quality](https://app.datacamp.com/learn/courses/introduction-to-data-quality)

**Critical for Decision-Making**: High-quality data is essential for informed business decisions, directly affecting trust and value in the data used.

**Key Activities for Quality**: Maintaining good data quality requires monitoring, timely issue resolution, and awareness of data's value among those who handle it.

**Benefits**: Good data quality provides competitive advantages and risk mitigation, enhancing customer service and preventing operational issues.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v75kmlq6v9jrpv5cdqjd.png)

### Data quality dimensions

Data completeness example:
![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vgpe5guycgeihtfoohfc.png)

Data validity example:
![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g988lhlbo0igw5fjzrhd.png)

Data uniqueness example:![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8d16pkmzgrtn9fr381zh.png)

Data consistency example:![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dgsc4maosr3ylaq11667.png)

Data accuracy example:![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xln47okyi4hnjtsh1ju7.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/an6hrr4ieckn4phsrw0u.png)

### Data quality rules examples

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/cqmao5w1sjysvtugva3t.png)

### Data quality roles

**Data Producers**: Individuals or systems responsible for creating, collecting, and managing data. Their duties include implementing and adhering to data quality rules, setting alerts for data issues, and fixing any identified problems.

**Data Consumers**: Users or systems that utilize data for analysis, reporting, or operational purposes. They are tasked with evaluating the quality of data before use, providing feedback on data quality rules based on business needs, and reporting any data quality issues to producers.

**Data Governance Team**: A group responsible for the oversight of the data quality program, including the establishment and enforcement of data quality policies and standards, monitoring data quality metrics, and ensuring the provision of necessary tools and training for maintaining data quality.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hjvukce5rytrsrewwn4g.png)

### Data profiling

Data profiling: The activity of running statistics on a data set to better understand the data and field dependencies

Examples:

- ﻿﻿How many records are in the data set?
- ﻿﻿What are the min and max values for a particular data element?
- ﻿﻿How many records have a particular data element populated?
- ﻿﻿When column A is populated, what other columns are also populated?

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c9gwnvabw67b966ntra6.png)

### Metadata

Attributes that describe data.

- ﻿﻿Used to organize and understand datasets and data elements
- ﻿﻿Used in the data quality process to determine the:
   o definition of a field
   o owner of a field
   o field's last update date

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/4p8p34gdvi1hpjub33k0.png)

**Data lineage**: A representation of how data moves in a pipeline, from where the data is entered in the source through each step in the data pipeline, until it is consumed.

- ﻿﻿Each layer has its own metadata
- ﻿﻿Used in the data quality process to determine where to implement a data quality rule

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/08pdj26uwc50vbgdea6b.png)

### Using Data lineage and Metadata

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/nh66q5ot5pyheaxzorey.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/adgbvtckbjvkrp7nggqq.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/nyy8o7ecilstty13yuf2.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/f4k7ksneitdbyb6je0e0.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j1itut6e9st4ze9c04qr.png)

### Data quality rules in action

**Detective data quality rule**: rule that monitors the data after it has already been loaded into the downstream target databases where it can be consumed. Ok if we can fix later and we can live with a low percentage of issues; we do not block data to be loaded.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iq5ou6nmy3vbs17a0d2a.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kqhjpcliq1uvmbcas84s.png)

**Preventative data quality rule:** stops the data from loading. Use it when the data is critical, or can be easily fixed, or impacts a high amount of records.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1p9yd18ki7x3e8a43j1y.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/l87outpvwv6dg9w00vyd.png)

### Anomaly detection

**Automated Data Monitoring**: Anomaly detection leverages machine learning algorithms to automatically identify potential data quality issues without needing constant human oversight, allowing for efficient monitoring of large datasets.

**Scalability and Insight**: It offers the ability to monitor data at scale, requiring minimal initial business knowledge to set up. Over time, it can detect data drift and provide insights into non-obvious changes in data patterns that may not be immediately apparent to the business.

**Complement to Traditional Rules**: While anomaly detection can automate and enhance data quality monitoring, especially in large datasets, it's most effective when used alongside traditional data quality rules, particularly in highly regulated industries that require rigorous data governance.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dxe6wz8hbkoudfdhzclr.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vzbiw1aievaq82egr73s.png)

Anomaly threshold alert example:
![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j2qb9j42jwd3ilmdnfbv.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mpausekkz73fq189ho17.png)

## [Introduction to Data Security](https://app.datacamp.com/learn/courses/introduction-to-data-security)

**Data Security Fundamentals**: Data security involves protecting digital information from unauthorized access, alteration, or destruction, focusing on ensuring confidentiality, integrity, and availability of data.

**Sensitive Data Identification**: Understanding and identifying sensitive data is crucial for effective data protection, guiding the selection of security controls and resource allocation.

**Proactive Approach and Continuous Learning**: Data security is an evolving field, requiring a comprehensive and proactive approach that includes staying informed about current threats, learning from past breaches, and fostering a culture that values data protection and awareness.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ijyg7jhscc9jttiq51za.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tn393tpb2yu6xkzwdeif.png)

### Compliance rules and regulations

**Compliance Fundamentals**: adhering to both legal and regulatory requirements designed to protect data confidentiality, integrity, and availability, safeguard personal information, and ensure responsible data handling by companies.

**Core Compliance Areas**: 

* proper data collection and lawful processing, 
* rights of individuals over their personal data, 
* measures to protect data from unauthorized access or breaches. 

**Key Regulations**: 

	* **GDPR** in the EU focus on privacy and data subject rights, 
	* **SOX** in the U.S. targets financial fraud and corporate accountability,
 * **CISA** Act encourages public-private cooperation against cyber threats. 
   Understanding these regulations helps in avoiding legal issues, protecting customer data, and maintaining trust.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5x9xq6kslaisu42acjyi.png)

**Classifying Data by Sensitivity**: Data is classified into public, internal, confidential, and top-secret categories, based on the potential impact of unauthorized access.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1w9qtjwvk54n3m5kyuhm.png)

### Voluntary regulatory frameworks

**Voluntary vs. Mandatory Frameworks**: developed by government or industry bodies, provide flexible guidelines for organizations to tailor their data security processes, unlike mandatory regulations which are legally binding and prescriptive.

**Examples of Frameworks**: 

* COBIT, 

* ISO 27001, 

* NIST Cybersecurity Framework (NIST CSF), 

  NIST CSF, in particular, is structured around five functions: identify, protect, detect, respond, and recover, offering a comprehensive approach to cybersecurity.

**Choosing and Implementing Frameworks**: The best framework for an organization depends on its specific needs, such as size, data types handled, industry, and risk profile. Many organizations adopt elements from multiple frameworks to create a hybrid approach that meets their unique requirements.

### Operational models 

Operational models offer specific, internally defined processes and rules to enhance data security by addressing particular challenges and protecting types of data. Five key models include:

1. **Zero Trust Architecture (ZTA)**: Adopts a "never trust, always verify" approach, emphasizing continuous verification and strict access controls to minimize unauthorized access.

2. **Separation of Duties (SoD)**: Divides tasks and privileges to prevent concentration of control, enhancing checks and balances, especially important in financial sectors.

3. **Principle of Least Privilege (PoLP)**: Limits user access to only what is necessary for their job functions, reducing security risks.

4. **Data Loss Prevention (DLP)**: Employs tools and practices to protect sensitive information from unauthorized access or theft, using monitoring, detection, and blocking mechanisms.

5. **Role-Based Control (RBC)**: Assigns access permissions based on job roles, streamlining access management and minimizing unauthorized access risks.

### Data protection measures

Data protection strategies are crucial for safeguarding sensitive information against unauthorized access or threats, structured within a comprehensive security ecosystem. Here's a summary:

1. **Hierarchy of Data Security**: 

   **Data protection measures** form the base, 

   **Operational models** implement these measures, 

   **Voluntary regulatory frameworks** offer guidance,

   **Compliance with legal regulations** ensures adherence to mandatory standards.

2. **Core Protection Measures**:

   - **Access Controls**: Serve as the first defense line, limiting who can see or use specific data, bolstered by strong passwords and multi-factor authentication.
   - **Encryption**: Essential for keeping data confidential, transforming readable data into a secure format that unauthorized users cannot understand.
   - **Secure Hardware Management**: Involves maintaining hardware securely to prevent risks associated with physical devices, including proper disposal.

3. **Special Considerations**:
   - **Cloud Computing**: Requires selecting providers with strong security practices and Cloud Data Loss Prevention tools for monitoring and protecting data.
   - **Generative AI**: Presents new data protection challenges, necessitating strategies to address risks like unintended data sharing or exposure.

## Introduction to Data literacy

The ability to read, work with, analyze, and communicate insights with data.

- ﻿﻿Reading data
  - ﻿﻿Identify data sources
  - ﻿﻿Collect data
  - ﻿﻿Manage data
- ﻿﻿Working with and analyzing data
  - Turn data into insights
  -  Data analytics:
    - Descriptive analytics 
    - Predictive analytics
- ﻿﻿Communicating insights with data
  - Demonstrate the insights
  - Present possible actions

### Analytics



![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qlx9pvxi8z1nam25r41t.png)

#### Descriptive analytics

- ﻿﻿Get to know the data
- ﻿﻿Investigate relationships in the data
- ﻿﻿Preparation for more advanced techniques

#### Diagnostic analytics

- ﻿﻿Find potential causes of events or reasons for behaviors
- ﻿﻿Investigate causal relationships
- ﻿﻿Suggest solutions based on the identified causes

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0xdnphxt7vte1eghh6yh.png)

#### Predictive analytics

- Anticipate most likely outcomes
- ﻿﻿Forecast a process or sequence
- ﻿﻿Estimate an unknown based on the information that is available

![image-20240417075752093](/Users/murat/Library/Application Support/typora-user-images/image-20240417075752093.png)

- Data is split into training and test set for building the predictive model
- ﻿﻿Predictions are interpreted and evaluated on the test data, using pre-determined metrics like accuracy (percentage of correct predictions)

#### Prescriptive analytics

- ﻿﻿Make informed, data-driven decisions
- ﻿﻿Optimize processes
- ﻿﻿Mitigate risks

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9bet28yqk24cqm6f21et.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/p7b4j8py8s6sgn8coe1l.png)

### Communicating insights

#### Visualization

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6yp0kid1lwpfrb517l4v.png)

McCandless technique

1. ﻿﻿﻿Introduce the visualization
2. ﻿﻿﻿Anticipate obvious questions
3. ﻿﻿﻿State the central insight
4. ﻿﻿﻿Provide supporting evidence
5. ﻿﻿﻿Closing statement

#### Storytelling

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mop428iyznouuugwesof.png)

Elements of a narrative

- ﻿﻿Characters: (﻿Stakeholders)
- ﻿﻿Problem: (Business problem)
- ﻿﻿Setting: (﻿Relevant background)
- ﻿﻿Plot: (﻿Central and supporting insights)
- ﻿﻿Resolution: (﻿Solution and recommendations)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/22juttfs90ojxvd6zd20.png)

#### 3 keys to communicating effectively

**Focus**

- ﻿﻿Select the right data
- ﻿﻿Select the right visualizations
- ﻿﻿Keep your central message in mind

Building your central message; describe in a few sentences the take-home message:

1. ﻿﻿﻿Problem

2. ﻿﻿﻿Insights

3. ﻿﻿﻿Impact

   

    *(Problem) Average math scores of students at University A are in decline for the last three years.*
    *(Insights) Data analysis indicates that more students enroll with a weaker starting knowledge of mathematics.*
    *(Impact) By organizing math summer schools, University A can help students better prepare for the expected knowledge level of mathematics.*

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pydze9lhmx6a7a16eebf.png)



**Structure**

- Use structuring techniques like storytelling and McCandless
- ﻿﻿Think about visual structure in the lay-out
- ﻿﻿Prepare an outline in advance and use this as a blueprint

Crafting an outline:

1. Introduction

   - ﻿﻿Data problem statement

   - ﻿﻿Context

   - Objectives

2. Body 
   * Data
   * Analysis 
   * Key findings

3. Conclusions

- ﻿﻿Insights
- ﻿﻿Recommendations

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3k01g7r23xbntkor4r7x.png)

**Form**

- ﻿﻿Medium: oral vs. written communication
- ﻿﻿Language: technical vs. non-technical
- ﻿﻿Scope: short- vs. long-form

To find the appropriate format: think about the purpose of your communication and its audience.

Thinking about your audience

- ﻿﻿Who are they?
- ﻿﻿What do they need to know?
- ﻿﻿What do they want?

*The University board:*

- ﻿﻿*Non-technical managers*
- ﻿﻿*Need to know key findings*
- ﻿﻿*Want specific recommendations*

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xhjezsgkyh32lygm8f7u.png)



## [Understanding Data Engineering](https://app.datacamp.com/learn/courses/understanding-data-engineering)

Data workflow (KEY):

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9l0po6md4murxdz2hkkx.png)

Data engineers deliver:

- ﻿﻿the correct data
- ﻿﻿in the right form
- ﻿﻿to the right people
- ﻿﻿as efficiently as possible

A data engineer's responsibilities:

- ﻿﻿Ingest data from different sources
- ﻿﻿Optimize databases for analysis
- ﻿﻿Remove corrupted data
- ﻿﻿Develop, construct, test and maintain data architectures

The five Vs of big data:

- ﻿﻿Volume (how much?)
- ﻿﻿Variety (what kind?)
- ﻿﻿Velocity (how frequent?)
- ﻿﻿Veracity (how accurate?)
- ﻿﻿Value (how useful?)

Data engineer vs data scientist responsibilities:

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fh0lxll62ggwngxgtlrq.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2vk0axo18xl9h3bdorsa.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vn41l2i4u64hlwuqcej4.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/we8gqtiya8r4trfq6glu.png)

ETL

- ﻿﻿Popular framework for designing data pipelines
- ﻿﻿1) Extract data
- ﻿﻿2) Transform extracted data
- ﻿﻿3) Load transformed data to another database

Data pipelines

- ﻿﻿Move data from one system to another
- ﻿﻿May follow ETL
- ﻿﻿Data may not be transformed
- ﻿﻿Data may be directly loaded in applications

### Data storage

Here's a brief summary of structured, semi-structured, and unstructured data, along with their differences:

1. **Structured Data**:
   - **Description**: Highly organized and easily searchable, structured data follows a strict schema, such as tables with defined columns and types. It's commonly stored in relational databases.
   - **Example**: Employee tables where each row is an employee and each column holds specific data (like team or role).
   - **Querying**: Utilizes languages such as SQL.

2. **Semi-Structured Data**:
   - **Description**: This data type is less rigid than structured data but still has some organizational properties. It does not fit neatly into tables as structured data does, but contains tags or markers to separate semantic elements and enforce hierarchies of records and fields.
   - **Example**: JSON files where each object can have a different number of fields or nested structures.
   - **Querying**: Typically managed in NoSQL databases, leveraging formats like JSON, XML, or YAML.

3. **Unstructured Data**:
   - **Description**: Lacks a predefined format or structure, making it more difficult to collect, process, and analyze. It includes formats like text, images, videos, and audio.
   - **Example**: Media files, texts, or social media posts.
   - **Storage and Analysis**: Often stored in data lakes, and requires advanced techniques like machine learning for processing and analysis.

**Differences**:

- **Schema and Organization**: Structured data is highly organized with a fixed schema, semi-structured data has a flexible schema with some organizational traits, and unstructured data has no specific format or structure.
- **Storage Systems**: Different storage systems are used for each, with structured data often in relational databases, semi-structured in NoSQL databases, and unstructured in data lakes or specialized file systems.
- **Processing Needs**: Structured data is easiest to work with using standard database management tools, semi-structured data requires more flexible processing approaches, and unstructured data often needs sophisticated techniques like machine learning to extract useful information.



![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wjz9exsc07rnzxqob8ss.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qcgfrpianynqsjo4j0xs.png)



### SQL Databases:

- **Function**: SQL (Structured Query Language) is crucial in data engineering, primarily used in relational database management systems (RDBMS) to query and manipulate structured data.

* **Usage in Data Engineering and Data Science**:

  - **Data Engineers**: Use SQL to create and manage database schemas, including defining table structures and data types. Example commands include `CREATE TABLE` where various columns are defined with specific data constraints.

  - **Data Scientists**: Utilize SQL to perform queries on these databases to extract, analyze, and manipulate data based on specific criteria. For instance, querying employee names from a table where their roles include "data".

* **Database Schema**:

   - **Structure**: Databases in an RDBMS are structured with multiple tables connected through relationships, such as linking albums, artists, and songs via unique IDs.
   - **Relational Nature**: These relationships allow for complex queries across multiple tables, illustrating the interconnectedness of data within an RDBMS.



### Data lake vs Data warehouse

Data lake:

- ﻿﻿Stores all the raw data
- ﻿﻿Can be petabytes (1 million GBs)
- ﻿﻿Stores all data structures
- ﻿﻿Cost-effective
- ﻿﻿Difficult to analyze
- ﻿﻿Requires an up-to-date data catalog
- ﻿﻿Used by data scientists
- ﻿﻿Big data, real-time analytics

Data warehouse:

- ﻿﻿Specific data for specific use
- ﻿﻿Relatively small
- ﻿﻿Stores mainly structured data
- ﻿﻿More costly to update
- ﻿﻿Optimized for data analysis
- ﻿﻿Also used by data analysts and business analysts
- ﻿﻿Ad-hoc, read-only queries

Data catalog for data lakes:

- ﻿﻿What is the source of this data?
- ﻿﻿Where is this data used?
- ﻿﻿Who is the owner of the data?
- ﻿﻿How often is this data updated?
- ﻿﻿Good practice in terms of data governance
- ﻿﻿Ensures reproducibility
- ﻿﻿No catalog --> data swamp

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n15ywleqwsk8up8l4dz4.png)

### Data processing

The act of converting raw data into meaningful, actionable information.

**Why Process Data?**

- **Cost Efficiency**: Processing optimizes storage, processing, and network costs by removing unnecessary data and compressing what remains.
- **Usability**: Transforms data into formats more useful for business operations, such as converting high-quality audio files into more network-friendly formats.

**Examples**:

- Artists upload high-quality audio files which are then converted to a lighter, slightly lower quality format for streaming.
- Metadata from music files is extracted and stored for easy access by analysts.
- Employee data is structured according to specific schemas for clarity and efficiency.

**Role of Data Engineers**:

- **Responsibilities**: Includes data manipulation, cleaning, and organizing to ensure data is ready for analysis. This involves rejecting corrupt files, managing missing metadata, and maintaining structured databases.
- **Optimization**: Improving database performance, e.g., by indexing data for quicker retrieval.

### Scheduling

Scheduling is crucial for automating and organizing the execution of tasks within a data pipeline. It involves setting up tasks to run at specific times or under certain conditions to efficiently manage data updates and processes. 

Understanding scheduling helps optimize the flow of data updates and processing, ensuring data systems are current and functional.

Key points include:

- **Types of Scheduling**: Tasks can be triggered manually, at scheduled times, or in response to certain conditions (sensor scheduling).
- **Data Pipeline Examples**: Updates to databases can be scheduled daily or triggered by specific events, such as the addition of a new employee.
- **Batch vs. Stream Processing**: Data can be processed in batches at set intervals or streamed in real time, depending on the need for immediacy.
- **Scheduling Tools**: Tools like Apache Airflow and Luigi help manage these tasks.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/x79c9m26faj33tv0nymk.png)

### Parallel computing

Parallel computing, or parallel processing, is a method used in modern data engineering to enhance memory management and processing power. It involves dividing a large task into smaller subtasks, which are then executed simultaneously across multiple computers.

This approach not only speeds up the process but also reduces the memory load on individual computers. However, it has some drawbacks, such as the cost of moving data and the time required to manage communication between the processes. 

## [Understanding Data Science](https://app.datacamp.com/learn/courses/understanding-data-science)

Data workflow (KEY):

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9l0po6md4murxdz2hkkx.png)

**Data engineer tools:**

- ﻿﻿SQL: ﻿to store and organize data
- ﻿﻿Java, Scala, or Python: to process data
- ﻿﻿Shell / Command line: to automate and run tasks
- ﻿﻿Cloud computing: ﻿﻿AWS, Azure, Google Cloud Platform

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/x29mrua4t0zb8ydbdmt5.png)

**Data analyst tools:**

- ﻿﻿SQL: retrieve and aggregate data
- Excel or Google Sheets: simple analysis
- ﻿﻿BI tools (Tableau, Power BI, Looker): dashboards and visualizations
- ﻿﻿May have Python or R: to clean and analyze data

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zy5oumtte4i4poy073hz.png)

**Data scientist tools**: 

- ﻿﻿SQL: retrieve and aggregate data
- ﻿﻿Python and/or R: data science libraries, e.g., pandas (Python) and tidyverse (R)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/o9elpak2eyue4wgxfnyg.png)

**Machine learning tools**:

- ﻿﻿Python and/or R: ﻿﻿Machine learning libraries, e.g. TensorFlow or Spark

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xeqoldu553tigol3yuoo.png)

### Data collection

- **Data Collection Importance:** Data is collected from various sources including web interactions, financial transactions, and through surveys to support data-driven decisions.
- **Sources of Data:** Includes both internal company data and external open data, accessible via methods like APIs and public records.
- **Utilization and Examples:** Data from sources like Twitter's API can be used for analyses such as sentiment tracking, while public records provide wide-ranging sectoral data.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pkb5nxr6j9xdvo8u7db5.png)

**Different data types**

- ﻿﻿Quantitative data
- ﻿﻿Qualitative data
- ﻿﻿Image data
- ﻿﻿Text data
- ﻿﻿Geospatial data
- ﻿﻿Network data

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/jns81ccuygtntkj0bqux.png)

#### Database examples: relational vs document

**Relational Database examples**

- The dates, times, subjects, and recipient addresses for all emails you ever sent
- Customer information for all students of a university, such as name, phone number, and location
- Sales data with customer IDs, dates, and purchase amounts for each transaction.
- Inventory lists with product IDs, descriptions, prices, and stock levels.
- Employee records with employee IDs, names, positions, salaries, and department codes.

**Document Database Examples**

- Text from various emails sent and received by you
- Images of different traffic events, including metadata about the image’s contents
- Blog posts with the text, author information, embedded images, and comments.
- Social media user profiles including user bio, posts, connections, and messages.
- Product catalog entries for an online store with detailed descriptions, reviews, and product specifications.

#### **Data pipeline**

- ﻿﻿Moves data into defined stages

- ﻿﻿Automated collection and storage

  - ﻿﻿Scheduled hourly, daily, weekly, etc
  - ﻿﻿Triggered by an event

- ﻿﻿Monitored with generated alerts

- ﻿﻿Necessary for big data projects

- ﻿﻿Data engineers work to customize solutions

- **Data Pipeline Mechanism**: ingestion, loading into a database, monitoring and customizing pipelines for efficient data flow, including the use of ETL (Extract, Transform, Load) frameworks.

  

TL example- Smart Home:** 

-  APIs and IoT devices are used to collect various data points for a smart home
- extracting data from different sources
- transforming it for consistency and relevance,
  - ﻿﻿Joining data sources into one data set
  - ﻿﻿Converting data structures to fit database schemas
  - ﻿﻿Removing irrelevant data
-  Loading it into a database for analysis.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/suf4to0n10tjxnwfdilk.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kjww7u3axxksvgrteagk.png)

### Data preparation

Raw data often contains errors, inconsistencies, and discrepancies that can distort analysis outcomes. Clean and well-structured data is essential to ensure accuracy and reliability in data analysis and decision-making.

- **Removing Duplicates:** Highlights the removal of duplicate data entries.
- **Assigning Unique IDs:** Suggests assigning unique identifiers to distinguish between similar entries.
- **Ensuring Homogeneity:** Stresses the importance of standardizing measurements and categorizations across the dataset.
- **Handling Missing Values:** Discusses methods to deal with missing data, including substitution with mean or median values, dropping, or ignoring if the algorithm permits.
- **Tidying Data:** Describes tidying as arranging data into a structure with observations as rows and variables as columns, 

**Exploratory Data Analysis (EDA**) involves previewing data, understanding data types, calculating descriptive statistics, identifying outliers, and visualizing trends to gain insights, 

### Exploration & Visualization

Interactive dashboards enhance data visualization by aggregating multiple graphs, allowing for comprehensive insights at a glance, and they can be created with BI tools without programming knowledge. These dashboards should be designed with purposeful use of color, consideration for colorblindness, readable fonts, and clear labeling to ensure clarity and avoid misleading representations. Interactivity adds a level of user engagement, allowing for personalized data exploration.

### Experimentation & Prediction

#### A/B testing

A/B Testing in data science is a controlled experiment involving two variants, A and B, to make decisions based on statistical evidence. 

1. Formulating a hypothesis, 
2. Collecting data, 
3. Running the test to a pre-determined sample size for robust results, 
4. Performing statistical tests to determine if observed differences are significant, not due to chance. If results are not statistically significant, it suggests that any differences in the tested variants are too small to be deemed important for the decision-making process.

#### Time series forecasting

Time series forecasting involves using historical data to predict future values over time, with models that may range from simple linear equations to complex deep learning algorithms. 

* commonly applied to metrics like unemployment rates, stock prices, or even social media trends. 
* Identify patterns such as seasonality, where certain trends repeat over time, and use statistical or machine learning techniques to create predictive models. 
* Confidence intervals are essential in these forecasts, providing a range within which future values are likely to fall, helping to buffer decisions against the unexpected.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3zhmry2x6kpn6tg6snun.png)

#### Supervised machine learning

- ﻿﻿Make a prediction based on data
- ﻿﻿Data has features and labels 
  - Label: what we want to predict
  - Features: data that might predict the label
- ﻿﻿Trained model can make predictions

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/nynrghuump7rldzbdy7h.png)

![image-20240422082720597](/Users/murat/Library/Application Support/typora-user-images/image-20240422082720597.png)

#### Clustering

Clustering is a form of unsupervised machine learning that groups data into clusters without predefined labels, allowing patterns to be identified in datasets. 

Unlike supervised learning that uses labeled data, clustering only requires data features, making it suitable when little is known about the dataset. 

The process involves selecting features, determining an initial number of clusters, and adjusting based on data insights and domain knowledge. 

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/esy8lh9xd4q5646yjhpj.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hik28c0evag2vu30mqv1.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dz136t94hu6lj5fxmm8n.png)

## [Understanding Data Visualization](https://app.datacamp.com/learn/courses/understanding-data-visualization)

### Visualizing distributions

#### Continuous and categorical variables

Continuous: usually numbers

• heights, temperatures, revenues

Categorical: usually text

• eye colors, countries, industry

Can be either

- ﻿﻿age is continuous, but age group is categorical
- ﻿﻿time is continuous, month of year is categorical

##### Histograms

When should you use a histogram?

1. ﻿﻿﻿If you have a single continuous variable.
2. ﻿﻿﻿You want to ask questions about the shape of its distribution.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/d2yvdsxbm66kmh1omoat.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/phu2n3zzplpsdilp2x21.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/x33uap9c8ft0nkcx6kh4.png)

### Box plots

When should you use a box plot?

1. ﻿﻿﻿When you have a continuous variable, split by a categorical variable.
2. ﻿﻿﻿When you want to compare the distributions of the continuous variable for each category.

Inter-quartile range (IQR) measures the variation in the "middle half" of the population (from the 25th percentile to the 75th percentile). That means that sorting by the IQR makes it easier to answer questions about how much variation there was among the "typical" population.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qfef3vcbbmcmtovhfzuc.png)

Box plots are useful when we want to compare many distributions at once.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/whxqx3c7y2o0v57nvsjh.png)

### Scatter plot  Scatter plot

- Scatter plots best represent the relationship between two quantitative variables
- ﻿﻿Plot the observations using an X-axis and Y-axis
- ﻿﻿Scatter plots show if the relationship between two variables is positive, negative, or relatively neutral.

When should you use a scatter plot?

1. ﻿﻿﻿You have two continuous variables.
2. ﻿﻿﻿You want to answer questions about the relationship between the two variables.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fwndqiqo2lmxpw50qzel.png)

### Line plots / Line chart

- Line charts work well with time series data
- ﻿﻿Compare and contrast performance over time
- ﻿﻿Trends become easy to spot and analyze

When should you use a line plot?

1. ﻿﻿﻿You have two continuous variables.
2. ﻿﻿﻿You want to answer questions about their relationship.
3. ﻿﻿﻿Consecutive observations are connected somehow.

Usually, but not always, the x-axis is dates or times.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/54tlnol9l8p8p1y80jyf.png)

### Bar chart / Bar plot

- Show counts and frequency by category
- ﻿﻿When a pie chart is used, try a bar chart instead
- ﻿﻿Bar charts require less effort from your audience

When should you use a bar plot?

Most common cases:

1. ﻿﻿﻿You have a categorical variable.
2. ﻿﻿﻿You want counts or percentages for each category.

Occasionally:

1. You want another numeric score for each category, and need to include zero in the plot.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/txqsmqtxmf40h1ogawd5.png)

### Dot plot 

When should you use a dot plot?

1. ﻿﻿﻿You have a categorical variable.
2. ﻿﻿﻿You want to display numeric scores for each category on a log scale, or
3. ﻿﻿﻿You want to display multiple numeric scores for each category.
4. ![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ocn4mpgin0pnuvm60gxc.png)

### Higher dimensions

× and y are not the only dimensions

Points also have these dimensions

- ﻿﻿color (best)
- ﻿﻿size
- ﻿﻿transparency
- ﻿﻿shape

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qzb2q4voq6q6227oivbb.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qxwlbrc8tiksvwchrmja.png)

Pie plots are generally a bad idea, except when the 360 angle is based on direction or time of day. Bar graph is almost always a better choice vs pie graph.

Dual axes are misleading; don't want 2 ys or 2 xs. Just use multiple panels instead.

## [Data Communication Concepts](https://app.datacamp.com/learn/courses/data-communication-concepts)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fzrp6p1pnj8xj5frqub1.png)

- ﻿﻿Data storytelling is important
  - ![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6n5vi7jzgdz5q75h2edv.png)
  - Narrative:![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8rgunuofcskltzknr373.png) 
- ﻿﻿Translate results for non-technical stakeholders
- Craft stories that impact the decision-making process
- Select right data and statistics
- ﻿﻿Audience persona
- ﻿﻿Choose appropriate visualization
- Types of reports
- ﻿﻿How to structure a clear report
- ﻿﻿Reproducibility
- Planning and building a presentation
- ﻿﻿Importance of practicing and rehearsing
- ﻿﻿Best practices and common mistakes when delivering a presentation

## [Data Storytelling Concepts](https://app.datacamp.com/learn/courses/data-storytelling-concepts)

1. Problem
   * ﻿﻿The problem itself
   * ﻿﻿An implied method of taking on the problem
   * ﻿﻿The scope of a problem
2. Rising action
3. Climax
4. Falling action
5. Resolution / Conclusion

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n6ums5e121i4b1irxaaw.png)
