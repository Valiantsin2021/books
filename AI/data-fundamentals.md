# Notes

## Sources

[Introduction to Data](https://app.datacamp.com/learn/courses/introduction-to-data)

[Introduction to Data Culture](https://app.datacamp.com/learn/courses/introduction-to-data-culture)

[Introduction to Data Quality](https://app.datacamp.com/learn/courses/introduction-to-data-quality)

[Introduction to Data Security](https://app.datacamp.com/learn/courses/introduction-to-data-security)

[Understanding Data Science](https://app.datacamp.com/learn/courses/understanding-data-science)

## [Introduction to Data](https://app.datacamp.com/learn/courses/introduction-to-data)

### Structured vs Unstructured data

Customer info, Financial transactions vs Email, Video

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2t881wvg8r6ouzmhuccf.png)

### Quantitative vs Qualitative data

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9e4zlksqc0eo77v2ohhx.png)

### Data Context

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v1qwhj8tyujpp72on2dn.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rppvutaumamzfdfvqod0.png)

### DIKW pyramid (4 layer model):

1. Data
2. Information
3. Knowledge
4. Wisdom

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/m38nhp3xkoamfk4ly4p2.png)

Start with raw data, organize it into information interpret it into knowledge & wisdom.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/307uvnuxqk92rl3yoc9k.png)

Wisdom: Wisdom means applied knowledge. Because it will probably rain tonight and make the park muddy, James' parents should plan some alternative games for their fifteen quests.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bo8aw9duugmvvxo068ck.png)

### Data decision making

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n1n1fti5knlcsi9d0dxz.png)

Asking the right question will:

- ﻿﻿Outline exact what you are looking to answer
- ﻿﻿Prevent scope creep
- ﻿﻿Ensure success throughout the rest of the process

Gather the right data for your question:

- ﻿﻿Data can live in multiple locations
- ﻿﻿Think ahead to your analysis, you can save time and effort now

Prepare data for analysis:

- ﻿﻿Clean bad to good data
- ﻿﻿Arrange data into expected structure for analysis
- ﻿﻿Sometimes most time consuming task

Analyzing data; decisions are based on data analysis. Here are some tools:

- ﻿﻿Python, R
- ﻿﻿Tableau, Power BI
- ﻿﻿Excel, Google Sheets

Making decisions:

- ﻿﻿The last step of the data-driven process
- ﻿﻿Better decisions based on data, especially when combined with personal experiences
- ﻿﻿Iterative process

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ycnzb27oe9cqlbmt5ca3.png)

- 

### Principles of data ethics

1. ﻿﻿﻿Permission for data collection
2. ﻿﻿﻿Transparency about the plan
3. ﻿﻿﻿Privacy of data
4. ﻿﻿﻿Good intentions
5. ﻿﻿﻿Consider the outcome

### What is the data life cycle?

- ﻿﻿Planning and collecting
- ﻿﻿Storing and managing
- ﻿﻿Cleaning and processing
- ﻿﻿Analyzing and visualizing
- ﻿﻿Sharing
- ﻿﻿Archiving/destroying

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/off9fgvju6c4zty14m30.png)

## [Introduction to Data Culture](https://app.datacamp.com/learn/courses/introduction-to-data-culture)

### Key components of data culture

- Making data-driven decisions
  - ﻿﻿Evaluate strategy
  - ﻿﻿Identify opportunities
  - ﻿﻿Measure success
  - ﻿﻿Evidence over intuition or opinion
- Prioritize data literacy: being able to read, interpret and analyze data
- Continuous experimentation and development

Benefits:

- ﻿﻿Enhanced decision-making

- ﻿﻿Improved operations

- Efficiency boost

  

### Data challenges

#### People related

Data silos:

- Occur when information is isolated within teams or departments
- ﻿﻿Hamper collaboration, limit organizational understanding
- ﻿﻿Example:
  * Ford's siloed data storage
- ﻿﻿Solutions:
  * Centralized data team/data management system

Data literacy:

- ﻿﻿Reluctance to gain new skills and adapt to changing data trend
- ﻿﻿Failure to invest in long-term commitment
- ﻿﻿Example:
  - ﻿﻿Assume all employees have the same level of data literacy
- ﻿﻿Solutions:
  - ﻿﻿Customized training program
  - ﻿﻿Continuously monitoring and refining

﻿﻿Data ethics:

- ﻿﻿Responsible, fair, and transparent data usage
- ﻿﻿Example:
   o Amazon's biased hiring algorithm
- ﻿﻿Solutions:
  - ﻿﻿Establish clear ethical guidelines
  - ﻿﻿Foster open discussion

#### Process related

Data quality:

- ﻿﻿Data needs to be ﻿accurate, reliable, up-to-date

* Example:
  - ﻿﻿Uber's "Greyball" resulted in severe consequences

* ﻿﻿Solutions:

  * Implement data validation process

  - ﻿﻿Leverage automated data cleansing tools

Data privacy & security:

- Extended data usage may lead to overlook of data privacy and data security
- ﻿﻿Protecting sensitive information maintains customer trust and avoid potential breaches
- ﻿﻿Example: 2017 Equifax data breach
- ﻿﻿Solutions:
  - ﻿﻿Implement access controls
  - ﻿﻿Apply encryption techniques
  - ﻿﻿Conduct regular security checks

### 5 level data maturity model

1. Awareness: the organization knows that data is important but lacks a plan for how to understand and analyze its data.
2. Adoption: some stakeholders within the organization have attempted to make data-informed decisions but the approach is often limited or siloed.
3. Standardization: achieved by breaking through the siloes between different pockets of the organization and acting as one unified vision.
4. Optimization: this is when the organizations try to improve efficiency by coordinating between different groups and data stream.
5. Innovation: a data culture is born and data's value, importance, and utility is realized at every level.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6usj9omw2w8svqusypt6.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/b8xrn213txy7hr5nsdme.png)

## [Introduction to Data Quality](https://app.datacamp.com/learn/courses/introduction-to-data-quality)

**Critical for Decision-Making**: High-quality data is essential for informed business decisions, directly affecting trust and value in the data used.

**Key Activities for Quality**: Maintaining good data quality requires monitoring, timely issue resolution, and awareness of data's value among those who handle it.

**Benefits**: Good data quality provides competitive advantages and risk mitigation, enhancing customer service and preventing operational issues.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v75kmlq6v9jrpv5cdqjd.png)

### Data quality dimensions

Data completeness example:
![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vgpe5guycgeihtfoohfc.png)

Data validity example:
![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g988lhlbo0igw5fjzrhd.png)

Data uniqueness example:![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8d16pkmzgrtn9fr381zh.png)

Data consistency example:![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dgsc4maosr3ylaq11667.png)

Data accuracy example:![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xln47okyi4hnjtsh1ju7.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/an6hrr4ieckn4phsrw0u.png)

### Data quality rules examples

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/cqmao5w1sjysvtugva3t.png)

### Data quality roles

**Data Producers**: Individuals or systems responsible for creating, collecting, and managing data. Their duties include implementing and adhering to data quality rules, setting alerts for data issues, and fixing any identified problems.

**Data Consumers**: Users or systems that utilize data for analysis, reporting, or operational purposes. They are tasked with evaluating the quality of data before use, providing feedback on data quality rules based on business needs, and reporting any data quality issues to producers.

**Data Governance Team**: A group responsible for the oversight of the data quality program, including the establishment and enforcement of data quality policies and standards, monitoring data quality metrics, and ensuring the provision of necessary tools and training for maintaining data quality.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hjvukce5rytrsrewwn4g.png)

### Data profiling

Data profiling: The activity of running statistics on a data set to better understand the data and field dependencies

Examples:

- ﻿﻿How many records are in the data set?
- ﻿﻿What are the min and max values for a particular data element?
- ﻿﻿How many records have a particular data element populated?
- ﻿﻿When column A is populated, what other columns are also populated?

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c9gwnvabw67b966ntra6.png)

### Metadata

Attributes that describe data.

- ﻿﻿Used to organize and understand datasets and data elements
- ﻿﻿Used in the data quality process to determine the:
   o definition of a field
   o owner of a field
   o field's last update date

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/4p8p34gdvi1hpjub33k0.png)

**Data lineage**: A representation of how data moves in a pipeline, from where the data is entered in the source through each step in the data pipeline, until it is consumed.

- ﻿﻿Each layer has its own metadata
- ﻿﻿Used in the data quality process to determine where to implement a data quality rule

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/08pdj26uwc50vbgdea6b.png)

### Using Data lineage and Metadata

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/nh66q5ot5pyheaxzorey.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/adgbvtckbjvkrp7nggqq.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/nyy8o7ecilstty13yuf2.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/f4k7ksneitdbyb6je0e0.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j1itut6e9st4ze9c04qr.png)

### Data quality rules in action

**Detective data quality rule**: rule that monitors the data after it has already been loaded into the downstream target databases where it can be consumed. Ok if we can fix later and we can live with a low percentage of issues; we do not block data to be loaded.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iq5ou6nmy3vbs17a0d2a.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kqhjpcliq1uvmbcas84s.png)

**Preventative data quality rule:** stops the data from loading. Use it when the data is critical, or can be easily fixed, or impacts a high amount of records.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1p9yd18ki7x3e8a43j1y.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/l87outpvwv6dg9w00vyd.png)

### Anomaly detection

**Automated Data Monitoring**: Anomaly detection leverages machine learning algorithms to automatically identify potential data quality issues without needing constant human oversight, allowing for efficient monitoring of large datasets.

**Scalability and Insight**: It offers the ability to monitor data at scale, requiring minimal initial business knowledge to set up. Over time, it can detect data drift and provide insights into non-obvious changes in data patterns that may not be immediately apparent to the business.

**Complement to Traditional Rules**: While anomaly detection can automate and enhance data quality monitoring, especially in large datasets, it's most effective when used alongside traditional data quality rules, particularly in highly regulated industries that require rigorous data governance.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dxe6wz8hbkoudfdhzclr.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vzbiw1aievaq82egr73s.png)

Anomaly threshold alert example:
![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j2qb9j42jwd3ilmdnfbv.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mpausekkz73fq189ho17.png)

## [Introduction to Data Security](https://app.datacamp.com/learn/courses/introduction-to-data-security)

**Data Security Fundamentals**: Data security involves protecting digital information from unauthorized access, alteration, or destruction, focusing on ensuring confidentiality, integrity, and availability of data.

**Sensitive Data Identification**: Understanding and identifying sensitive data is crucial for effective data protection, guiding the selection of security controls and resource allocation.

**Proactive Approach and Continuous Learning**: Data security is an evolving field, requiring a comprehensive and proactive approach that includes staying informed about current threats, learning from past breaches, and fostering a culture that values data protection and awareness.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ijyg7jhscc9jttiq51za.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tn393tpb2yu6xkzwdeif.png)

### Compliance rules and regulations

**Compliance Fundamentals**: adhering to both legal and regulatory requirements designed to protect data confidentiality, integrity, and availability, safeguard personal information, and ensure responsible data handling by companies.

**Core Compliance Areas**: 

* proper data collection and lawful processing, 
* rights of individuals over their personal data, 
* measures to protect data from unauthorized access or breaches. 

**Key Regulations**: 

	* **GDPR** in the EU focus on privacy and data subject rights, 
	* **SOX** in the U.S. targets financial fraud and corporate accountability,
 * **CISA** Act encourages public-private cooperation against cyber threats. 
   Understanding these regulations helps in avoiding legal issues, protecting customer data, and maintaining trust.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5x9xq6kslaisu42acjyi.png)

**Classifying Data by Sensitivity**: Data is classified into public, internal, confidential, and top-secret categories, based on the potential impact of unauthorized access.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1w9qtjwvk54n3m5kyuhm.png)

### Voluntary regulatory frameworks

**Voluntary vs. Mandatory Frameworks**: developed by government or industry bodies, provide flexible guidelines for organizations to tailor their data security processes, unlike mandatory regulations which are legally binding and prescriptive.

**Examples of Frameworks**: 

* COBIT, 

* ISO 27001, 

* NIST Cybersecurity Framework (NIST CSF), 

  NIST CSF, in particular, is structured around five functions: identify, protect, detect, respond, and recover, offering a comprehensive approach to cybersecurity.

**Choosing and Implementing Frameworks**: The best framework for an organization depends on its specific needs, such as size, data types handled, industry, and risk profile. Many organizations adopt elements from multiple frameworks to create a hybrid approach that meets their unique requirements.

### Operational models 

Operational models offer specific, internally defined processes and rules to enhance data security by addressing particular challenges and protecting types of data. Five key models include:

1. **Zero Trust Architecture (ZTA)**: Adopts a "never trust, always verify" approach, emphasizing continuous verification and strict access controls to minimize unauthorized access.

2. **Separation of Duties (SoD)**: Divides tasks and privileges to prevent concentration of control, enhancing checks and balances, especially important in financial sectors.

3. **Principle of Least Privilege (PoLP)**: Limits user access to only what is necessary for their job functions, reducing security risks.

4. **Data Loss Prevention (DLP)**: Employs tools and practices to protect sensitive information from unauthorized access or theft, using monitoring, detection, and blocking mechanisms.

5. **Role-Based Control (RBC)**: Assigns access permissions based on job roles, streamlining access management and minimizing unauthorized access risks.

### Data protection measures

Data protection strategies are crucial for safeguarding sensitive information against unauthorized access or threats, structured within a comprehensive security ecosystem. Here's a summary:

1. **Hierarchy of Data Security**: 

   **Data protection measures** form the base, 

   **Operational models** implement these measures, 

   **Voluntary regulatory frameworks** offer guidance,

   **Compliance with legal regulations** ensures adherence to mandatory standards.

2. **Core Protection Measures**:

   - **Access Controls**: Serve as the first defense line, limiting who can see or use specific data, bolstered by strong passwords and multi-factor authentication.
   - **Encryption**: Essential for keeping data confidential, transforming readable data into a secure format that unauthorized users cannot understand.
   - **Secure Hardware Management**: Involves maintaining hardware securely to prevent risks associated with physical devices, including proper disposal.

3. **Special Considerations**:
   - **Cloud Computing**: Requires selecting providers with strong security practices and Cloud Data Loss Prevention tools for monitoring and protecting data.
   - **Generative AI**: Presents new data protection challenges, necessitating strategies to address risks like unintended data sharing or exposure.

## Introduction to Data literacy

The ability to read, work with, analyze, and communicate insights with data.

- ﻿﻿Reading data
  - ﻿﻿Identify data sources
  - ﻿﻿Collect data
  - ﻿﻿Manage data
- ﻿﻿Working with and analyzing data
  - Turn data into insights
  -  Data analytics:
    - Descriptive analytics 
    - Predictive analytics
- ﻿﻿Communicating insights with data
  - Demonstrate the insights
  - Present possible actions

### Analytics



![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qlx9pvxi8z1nam25r41t.png)

#### Descriptive analytics

- ﻿﻿Get to know the data
- ﻿﻿Investigate relationships in the data
- ﻿﻿Preparation for more advanced techniques

#### Diagnostic analytics

- ﻿﻿Find potential causes of events or reasons for behaviors
- ﻿﻿Investigate causal relationships
- ﻿﻿Suggest solutions based on the identified causes

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0xdnphxt7vte1eghh6yh.png)

#### Predictive analytics

- Anticipate most likely outcomes
- ﻿﻿Forecast a process or sequence
- ﻿﻿Estimate an unknown based on the information that is available

![image-20240417075752093](/Users/murat/Library/Application Support/typora-user-images/image-20240417075752093.png)

- Data is split into training and test set for building the predictive model
- ﻿﻿Predictions are interpreted and evaluated on the test data, using pre-determined metrics like accuracy (percentage of correct predictions)

#### Prescriptive analytics

- ﻿﻿Make informed, data-driven decisions
- ﻿﻿Optimize processes
- ﻿﻿Mitigate risks

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9bet28yqk24cqm6f21et.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/p7b4j8py8s6sgn8coe1l.png)

### Communicating insights

#### Visualization

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6yp0kid1lwpfrb517l4v.png)

McCandless technique

1. ﻿﻿﻿Introduce the visualization
2. ﻿﻿﻿Anticipate obvious questions
3. ﻿﻿﻿State the central insight
4. ﻿﻿﻿Provide supporting evidence
5. ﻿﻿﻿Closing statement

#### Storytelling

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mop428iyznouuugwesof.png)

Elements of a narrative

- ﻿﻿Characters: (﻿Stakeholders)
- ﻿﻿Problem: (Business problem)
- ﻿﻿Setting: (﻿Relevant background)
- ﻿﻿Plot: (﻿Central and supporting insights)
- ﻿﻿Resolution: (﻿Solution and recommendations)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/22juttfs90ojxvd6zd20.png)

#### 3 keys to communicating effectively

**Focus**

- ﻿﻿Select the right data
- ﻿﻿Select the right visualizations
- ﻿﻿Keep your central message in mind

Building your central message; describe in a few sentences the take-home message:

1. ﻿﻿﻿Problem

2. ﻿﻿﻿Insights

3. ﻿﻿﻿Impact

   

    *(Problem) Average math scores of students at University A are in decline for the last three years.*
    *(Insights) Data analysis indicates that more students enroll with a weaker starting knowledge of mathematics.*
    *(Impact) By organizing math summer schools, University A can help students better prepare for the expected knowledge level of mathematics.*

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pydze9lhmx6a7a16eebf.png)



**Structure**

- Use structuring techniques like storytelling and McCandless
- ﻿﻿Think about visual structure in the lay-out
- ﻿﻿Prepare an outline in advance and use this as a blueprint

Crafting an outline:

1. Introduction

   - ﻿﻿Data problem statement

   - ﻿﻿Context

   - Objectives

2. Body 
   * Data
   * Analysis 
   * Key findings

3. Conclusions

- ﻿﻿Insights
- ﻿﻿Recommendations

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3k01g7r23xbntkor4r7x.png)

**Form**

- ﻿﻿Medium: oral vs. written communication
- ﻿﻿Language: technical vs. non-technical
- ﻿﻿Scope: short- vs. long-form

To find the appropriate format: think about the purpose of your communication and its audience.

Thinking about your audience

- ﻿﻿Who are they?
- ﻿﻿What do they need to know?
- ﻿﻿What do they want?

*The University board:*

- ﻿﻿*Non-technical managers*
- ﻿﻿*Need to know key findings*
- ﻿﻿*Want specific recommendations*

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xhjezsgkyh32lygm8f7u.png)

## [Understanding Data Science](https://app.datacamp.com/learn/courses/understanding-data-science)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9l0po6md4murxdz2hkkx.png)

**Data engineer tools:**

- ﻿﻿SQL: ﻿to store and organize data
- ﻿﻿Java, Scala, or Python: to process data
- ﻿﻿Shell / Command line: to automate and run tasks
- ﻿﻿Cloud computing: ﻿﻿AWS, Azure, Google Cloud Platform

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/x29mrua4t0zb8ydbdmt5.png)

**Data analyst tools:**

- ﻿﻿SQL: retrieve and aggregate data
- Excel or Google Sheets: simple analysis
- ﻿﻿BI tools (Tableau, Power BI, Looker): dashboards and visualizations
- ﻿﻿May have Python or R: to clean and analyze data

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zy5oumtte4i4poy073hz.png)

**Data scientist tools**: 

- ﻿﻿SQL: retrieve and aggregate data
- ﻿﻿Python and/or R: data science libraries, e.g., pandas (Python) and tidyverse (R)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/o9elpak2eyue4wgxfnyg.png)

**Machine learning tools**:

- ﻿﻿Python and/or R: ﻿﻿Machine learning libraries, e.g. TensorFlow or Spark

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xeqoldu553tigol3yuoo.png)

### Data collection

- **Data Collection Importance:** Data is collected from various sources including web interactions, financial transactions, and through surveys to support data-driven decisions.
- **Sources of Data:** Includes both internal company data and external open data, accessible via methods like APIs and public records.
- **Utilization and Examples:** Data from sources like Twitter's API can be used for analyses such as sentiment tracking, while public records provide wide-ranging sectoral data.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pkb5nxr6j9xdvo8u7db5.png)

**Different data types**

- ﻿﻿Quantitative data
- ﻿﻿Qualitative data
- ﻿﻿Image data
- ﻿﻿Text data
- ﻿﻿Geospatial data
- ﻿﻿Network data

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/jns81ccuygtntkj0bqux.png)

#### Database examples: relational vs document

**Relational Database examples**

- The dates, times, subjects, and recipient addresses for all emails you ever sent
- Customer information for all students of a university, such as name, phone number, and location
- Sales data with customer IDs, dates, and purchase amounts for each transaction.
- Inventory lists with product IDs, descriptions, prices, and stock levels.
- Employee records with employee IDs, names, positions, salaries, and department codes.

**Document Database Examples**

- Text from various emails sent and received by you
- Images of different traffic events, including metadata about the image’s contents
- Blog posts with the text, author information, embedded images, and comments.
- Social media user profiles including user bio, posts, connections, and messages.
- Product catalog entries for an online store with detailed descriptions, reviews, and product specifications.

#### **Data pipeline**

- ﻿﻿Moves data into defined stages

- ﻿﻿Automated collection and storage

  - ﻿﻿Scheduled hourly, daily, weekly, etc
  - ﻿﻿Triggered by an event

- ﻿﻿Monitored with generated alerts

- ﻿﻿Necessary for big data projects

- ﻿﻿Data engineers work to customize solutions

- **Data Pipeline Mechanism**: ingestion, loading into a database, monitoring and customizing pipelines for efficient data flow, including the use of ETL (Extract, Transform, Load) frameworks.

  

TL example- Smart Home:** 

-  APIs and IoT devices are used to collect various data points for a smart home
- extracting data from different sources
- transforming it for consistency and relevance,
  - ﻿﻿Joining data sources into one data set
  - ﻿﻿Converting data structures to fit database schemas
  - ﻿﻿Removing irrelevant data
-  Loading it into a database for analysis.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/suf4to0n10tjxnwfdilk.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kjww7u3axxksvgrteagk.png)

### Data preparation

Raw data often contains errors, inconsistencies, and discrepancies that can distort analysis outcomes. Clean and well-structured data is essential to ensure accuracy and reliability in data analysis and decision-making.

- **Removing Duplicates:** Highlights the removal of duplicate data entries.
- **Assigning Unique IDs:** Suggests assigning unique identifiers to distinguish between similar entries.
- **Ensuring Homogeneity:** Stresses the importance of standardizing measurements and categorizations across the dataset.
- **Handling Missing Values:** Discusses methods to deal with missing data, including substitution with mean or median values, dropping, or ignoring if the algorithm permits.
- **Tidying Data:** Describes tidying as arranging data into a structure with observations as rows and variables as columns, 

**Exploratory Data Analysis (EDA**) involves previewing data, understanding data types, calculating descriptive statistics, identifying outliers, and visualizing trends to gain insights, 

### Exploration & Visualization

Interactive dashboards enhance data visualization by aggregating multiple graphs, allowing for comprehensive insights at a glance, and they can be created with BI tools without programming knowledge. These dashboards should be designed with purposeful use of color, consideration for colorblindness, readable fonts, and clear labeling to ensure clarity and avoid misleading representations. Interactivity adds a level of user engagement, allowing for personalized data exploration.

### Experimentation & Prediction

#### A/B testing

A/B Testing in data science is a controlled experiment involving two variants, A and B, to make decisions based on statistical evidence. 

1. Formulating a hypothesis, 
2. Collecting data, 
3. Running the test to a pre-determined sample size for robust results, 
4. Performing statistical tests to determine if observed differences are significant, not due to chance. If results are not statistically significant, it suggests that any differences in the tested variants are too small to be deemed important for the decision-making process.

#### Time series forecasting

Time series forecasting involves using historical data to predict future values over time, with models that may range from simple linear equations to complex deep learning algorithms. 

* commonly applied to metrics like unemployment rates, stock prices, or even social media trends. 
* Identify patterns such as seasonality, where certain trends repeat over time, and use statistical or machine learning techniques to create predictive models. 
* Confidence intervals are essential in these forecasts, providing a range within which future values are likely to fall, helping to buffer decisions against the unexpected.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3zhmry2x6kpn6tg6snun.png)

#### Supervised machine learning

- ﻿﻿Make a prediction based on data
- ﻿﻿Data has features and labels 
  - Label: what we want to predict
  - Features: data that might predict the label
- ﻿﻿Trained model can make predictions

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/nynrghuump7rldzbdy7h.png)

![image-20240422082720597](/Users/murat/Library/Application Support/typora-user-images/image-20240422082720597.png)

#### Clustering

Clustering is a form of unsupervised machine learning that groups data into clusters without predefined labels, allowing patterns to be identified in datasets. 

Unlike supervised learning that uses labeled data, clustering only requires data features, making it suitable when little is known about the dataset. 

The process involves selecting features, determining an initial number of clusters, and adjusting based on data insights and domain knowledge. 

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/esy8lh9xd4q5646yjhpj.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hik28c0evag2vu30mqv1.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dz136t94hu6lj5fxmm8n.png)
