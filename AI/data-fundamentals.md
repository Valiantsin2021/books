# Notes

## Sources

[Introduction to Data](https://app.datacamp.com/learn/courses/introduction-to-data)

[Introduction to Data Culture](https://app.datacamp.com/learn/courses/introduction-to-data-culture)

[Introduction to Data Quality](https://app.datacamp.com/learn/courses/introduction-to-data-quality)

[Introduction to Data Security](https://app.datacamp.com/learn/courses/introduction-to-data-security)

## [Introduction to Data](https://app.datacamp.com/learn/courses/introduction-to-data)

### Structured vs Unstructured data

Customer info, Financial transactions vs Email, Video

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2t881wvg8r6ouzmhuccf.png)

### Quantitative vs Qualitative data

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9e4zlksqc0eo77v2ohhx.png)

### Data Context

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v1qwhj8tyujpp72on2dn.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rppvutaumamzfdfvqod0.png)

### DIKW pyramid (4 layer model):

1. Data
2. Information
3. Knowledge
4. Wisdom

Start with raw data, organize it into information interpret it into knowledge & wisdom.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/307uvnuxqk92rl3yoc9k.png)

Wisdom: Because it will probably rain tonight and make the park muddy, James' parents should plan some alternative games for their fifteen quests.

### Data decision making

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n1n1fti5knlcsi9d0dxz.png)

Asking the right question will:

- ﻿﻿Outline exact what you are looking to answer
- ﻿﻿Prevent scope creep
- ﻿﻿Ensure success throughout the rest of the process

Gather the right data for your question:

- ﻿﻿Data can live in multiple locations
- ﻿﻿Think ahead to your analysis, you can save time and effort now

Prepare data for analysis:

- ﻿﻿Clean bad to good data
- ﻿﻿Arrange data into expected structure for analysis
- ﻿﻿Sometimes most time consuming task

Analyzing data; decisions are based on data analysis. Here are some tools:

- ﻿﻿Python, R
- ﻿﻿Tableau, Power BI
- ﻿﻿Excel, Google Sheets

Making decisions:

- ﻿﻿The last step of the data-driven process
- ﻿﻿Better decisions based on data, especially when combined with personal experiences
- ﻿﻿Iterative process

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ycnzb27oe9cqlbmt5ca3.png)

- 

### Principles of data ethics

1. ﻿﻿﻿Permission for data collection
2. ﻿﻿﻿Transparency about the plan
3. ﻿﻿﻿Privacy of data
4. ﻿﻿﻿Good intentions
5. ﻿﻿﻿Consider the outcome

### What is the data life cycle?

- ﻿﻿Planning and collecting
- ﻿﻿Storing and managing
- ﻿﻿Cleaning and processing
- ﻿﻿Analyzing and visualizing
- ﻿﻿Sharing
- ﻿﻿Archiving/destroying

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/off9fgvju6c4zty14m30.png)

## [Introduction to Data Culture](https://app.datacamp.com/learn/courses/introduction-to-data-culture)

### Key components of data culture

- Making data-driven decisions
  - ﻿﻿Evaluate strategy
  - ﻿﻿Identify opportunities
  - ﻿﻿Measure success
  - ﻿﻿Evidence over intuition or opinion
- Prioritize data literacy: being able to read, interpret and analyze data
- Continuous experimentation and development

Benefits:

- ﻿﻿Enhanced decision-making

- ﻿﻿Improved operations

- Efficiency boost

  

### Data challenges

#### People related

Data silos:

- Occur when information is isolated within teams or departments
- ﻿﻿Hamper collaboration, limit organizational understanding
- ﻿﻿Example:
  * Ford's siloed data storage
- ﻿﻿Solutions:
  * Centralized data team/data management system

Data literacy:

- ﻿﻿Reluctance to gain new skills and adapt to changing data trend
- ﻿﻿Failure to invest in long-term commitment
- ﻿﻿Example:
  - ﻿﻿Assume all employees have the same level of data literacy
- ﻿﻿Solutions:
  - ﻿﻿Customized training program
  - ﻿﻿Continuously monitoring and refining

﻿﻿Data ethics:

- ﻿﻿Responsible, fair, and transparent data usage
- ﻿﻿Example:
   o Amazon's biased hiring algorithm
- ﻿﻿Solutions:
  - ﻿﻿Establish clear ethical guidelines
  - ﻿﻿Foster open discussion

#### Process related

Data quality:

- ﻿﻿Data needs to be ﻿accurate, reliable, up-to-date

* Example:
  - ﻿﻿Uber's "Greyball" resulted in severe consequences

* ﻿﻿Solutions:

  * Implement data validation process

  - ﻿﻿Leverage automated data cleansing tools

Data privacy & security:

- Extended data usage may lead to overlook of data privacy and data security
- ﻿﻿Protecting sensitive information maintains customer trust and avoid potential breaches
- ﻿﻿Example: 2017 Equifax data breach
- ﻿﻿Solutions:
  - ﻿﻿Implement access controls
  - ﻿﻿Apply encryption techniques
  - ﻿﻿Conduct regular security checks

### 5 level data maturity model

1. Awareness: the organization knows that data is important but lacks a plan for how to understand and analyze its data.
2. Adoption: some stakeholders within the organization have attempted to make data-informed decisions but the approach is often limited or siloed.
3. Standardization: achieved by breaking through the siloes between different pockets of the organization and acting as one unified vision.
4. Optimization: this is when the organizations try to improve efficiency by coordinating between different groups and data stream.
5. Innovation: a data culture is born and data's value, importance, and utility is realized at every level.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6usj9omw2w8svqusypt6.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/b8xrn213txy7hr5nsdme.png)

## [Introduction to Data Quality](https://app.datacamp.com/learn/courses/introduction-to-data-quality)

**Critical for Decision-Making**: High-quality data is essential for informed business decisions, directly affecting trust and value in the data used.

**Key Activities for Quality**: Maintaining good data quality requires monitoring, timely issue resolution, and awareness of data's value among those who handle it.

**Benefits**: Good data quality provides competitive advantages and risk mitigation, enhancing customer service and preventing operational issues.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v75kmlq6v9jrpv5cdqjd.png)

### Data quality dimensions

Data completeness example:
![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vgpe5guycgeihtfoohfc.png)

Data validity example:
![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g988lhlbo0igw5fjzrhd.png)

Data uniqueness example:![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8d16pkmzgrtn9fr381zh.png)

Data consistency example:![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dgsc4maosr3ylaq11667.png)

Data accuracy example:![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xln47okyi4hnjtsh1ju7.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/an6hrr4ieckn4phsrw0u.png)

### Data quality rules examples

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/cqmao5w1sjysvtugva3t.png)

### Data quality roles

**Data Producers**: Individuals or systems responsible for creating, collecting, and managing data. Their duties include implementing and adhering to data quality rules, setting alerts for data issues, and fixing any identified problems.

**Data Consumers**: Users or systems that utilize data for analysis, reporting, or operational purposes. They are tasked with evaluating the quality of data before use, providing feedback on data quality rules based on business needs, and reporting any data quality issues to producers.

**Data Governance Team**: A group responsible for the oversight of the data quality program, including the establishment and enforcement of data quality policies and standards, monitoring data quality metrics, and ensuring the provision of necessary tools and training for maintaining data quality.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hjvukce5rytrsrewwn4g.png)

### Data profiling

Data profiling: The activity of running statistics on a data set to better understand the data and field dependencies

Examples:

- ﻿﻿How many records are in the data set?
- ﻿﻿What are the min and max values for a particular data element?
- ﻿﻿How many records have a particular data element populated?
- ﻿﻿When column A is populated, what other columns are also populated?

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c9gwnvabw67b966ntra6.png)

### Metadata

Attributes that describe data.

- ﻿﻿Used to organize and understand datasets and data elements
- ﻿﻿Used in the data quality process to determine the:
   o definition of a field
   o owner of a field
   o field's last update date

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/4p8p34gdvi1hpjub33k0.png)

**Data lineage**: A representation of how data moves in a pipeline, from where the data is entered in the source through each step in the data pipeline, until it is consumed.

- ﻿﻿Each layer has its own metadata
- ﻿﻿Used in the data quality process to determine where to implement a data quality rule

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/08pdj26uwc50vbgdea6b.png)

### Using Data lineage and Metadata

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/nh66q5ot5pyheaxzorey.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/adgbvtckbjvkrp7nggqq.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/nyy8o7ecilstty13yuf2.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/f4k7ksneitdbyb6je0e0.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j1itut6e9st4ze9c04qr.png)

### Data quality rules in action

**Detective data quality rule**: rule that monitors the data after it has already been loaded into the downstream target databases where it can be consumed. Ok if we can fix later and we can live with a low percentage of issues; we do not block data to be loaded.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iq5ou6nmy3vbs17a0d2a.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kqhjpcliq1uvmbcas84s.png)

**Preventative data quality rule:** stops the data from loading. Use it when the data is critical, or can be easily fixed, or impacts a high amount of records.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1p9yd18ki7x3e8a43j1y.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/l87outpvwv6dg9w00vyd.png)

### Anomaly detection

**Automated Data Monitoring**: Anomaly detection leverages machine learning algorithms to automatically identify potential data quality issues without needing constant human oversight, allowing for efficient monitoring of large datasets.

**Scalability and Insight**: It offers the ability to monitor data at scale, requiring minimal initial business knowledge to set up. Over time, it can detect data drift and provide insights into non-obvious changes in data patterns that may not be immediately apparent to the business.

**Complement to Traditional Rules**: While anomaly detection can automate and enhance data quality monitoring, especially in large datasets, it's most effective when used alongside traditional data quality rules, particularly in highly regulated industries that require rigorous data governance.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dxe6wz8hbkoudfdhzclr.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vzbiw1aievaq82egr73s.png)

Anomaly threshold alert example:
![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j2qb9j42jwd3ilmdnfbv.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/mpausekkz73fq189ho17.png)

## [Introduction to Data Security](https://app.datacamp.com/learn/courses/introduction-to-data-security)

**Data Security Fundamentals**: Data security involves protecting digital information from unauthorized access, alteration, or destruction, focusing on ensuring confidentiality, integrity, and availability of data.

**Sensitive Data Identification**: Understanding and identifying sensitive data is crucial for effective data protection, guiding the selection of security controls and resource allocation.

**Proactive Approach and Continuous Learning**: Data security is an evolving field, requiring a comprehensive and proactive approach that includes staying informed about current threats, learning from past breaches, and fostering a culture that values data protection and awareness.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ijyg7jhscc9jttiq51za.png)

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tn393tpb2yu6xkzwdeif.png)

### Compliance rules and regulations

**Compliance Fundamentals**: adhering to both legal and regulatory requirements designed to protect data confidentiality, integrity, and availability, safeguard personal information, and ensure responsible data handling by companies.

**Core Compliance Areas**: 

* proper data collection and lawful processing, 
* rights of individuals over their personal data, 
* measures to protect data from unauthorized access or breaches. 

**Key Regulations**: 

	* **GDPR** in the EU focus on privacy and data subject rights, 
	* **SOX** in the U.S. targets financial fraud and corporate accountability,
 * **CISA** Act encourages public-private cooperation against cyber threats. 
   Understanding these regulations helps in avoiding legal issues, protecting customer data, and maintaining trust.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5x9xq6kslaisu42acjyi.png)

**Classifying Data by Sensitivity**: Data is classified into public, internal, confidential, and top-secret categories, based on the potential impact of unauthorized access.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1w9qtjwvk54n3m5kyuhm.png)

### Voluntary regulatory frameworks

**Voluntary vs. Mandatory Frameworks**: developed by government or industry bodies, provide flexible guidelines for organizations to tailor their data security processes, unlike mandatory regulations which are legally binding and prescriptive.

**Examples of Frameworks**: 

* COBIT, 

* ISO 27001, 

* NIST Cybersecurity Framework (NIST CSF), 

  NIST CSF, in particular, is structured around five functions: identify, protect, detect, respond, and recover, offering a comprehensive approach to cybersecurity.

**Choosing and Implementing Frameworks**: The best framework for an organization depends on its specific needs, such as size, data types handled, industry, and risk profile. Many organizations adopt elements from multiple frameworks to create a hybrid approach that meets their unique requirements.

### Operational models 

Operational models offer specific, internally defined processes and rules to enhance data security by addressing particular challenges and protecting types of data. Five key models include:

1. **Zero Trust Architecture (ZTA)**: Adopts a "never trust, always verify" approach, emphasizing continuous verification and strict access controls to minimize unauthorized access.

2. **Separation of Duties (SoD)**: Divides tasks and privileges to prevent concentration of control, enhancing checks and balances, especially important in financial sectors.

3. **Principle of Least Privilege (PoLP)**: Limits user access to only what is necessary for their job functions, reducing security risks.

4. **Data Loss Prevention (DLP)**: Employs tools and practices to protect sensitive information from unauthorized access or theft, using monitoring, detection, and blocking mechanisms.

5. **Role-Based Control (RBC)**: Assigns access permissions based on job roles, streamlining access management and minimizing unauthorized access risks.

### Data protection measures

Data protection strategies are crucial for safeguarding sensitive information against unauthorized access or threats, structured within a comprehensive security ecosystem. Here's a summary:

1. **Hierarchy of Data Security**: 

   **Data protection measures** form the base, 

   **Operational models** implement these measures, 

   **Voluntary regulatory frameworks** offer guidance,

   **Compliance with legal regulations** ensures adherence to mandatory standards.

2. **Core Protection Measures**:

   - **Access Controls**: Serve as the first defense line, limiting who can see or use specific data, bolstered by strong passwords and multi-factor authentication.
   - **Encryption**: Essential for keeping data confidential, transforming readable data into a secure format that unauthorized users cannot understand.
   - **Secure Hardware Management**: Involves maintaining hardware securely to prevent risks associated with physical devices, including proper disposal.

3. **Special Considerations**:
   - **Cloud Computing**: Requires selecting providers with strong security practices and Cloud Data Loss Prevention tools for monitoring and protecting data.
   - **Generative AI**: Presents new data protection challenges, necessitating strategies to address risks like unintended data sharing or exposure.

